import requests
import argparse


def read_urls_from_file(filename):
    try:
        with open(filename, 'r') as file:
            urls = file.readlines()
            urls = [url.strip() for url in urls]
            return urls
    except FileNotFoundError:
        print(f"File '{filename}' not found.")
        return []

def main(filename):
    urls = read_urls_from_file(filename)
    timeout_seconds = 10
    NormalStatusCodeURL = open('200_OK_URL.txt','w')
    RedirectStatusCodeURL = open('302_OK_URL.txt','w')
    ForbiddenStatusCodeURL = open('403_OK_URL.txt','w')
    InternalServerError = open('500_OK_URL.txt','w')

    if urls:
        print("Validating URLs...")
        for url in urls:
            PreparedURL = 'https://'+ str(url)
            try:
                response = requests.head(PreparedURL,timeout=timeout_seconds)
                if response.status_code == 200:
                    # Write to File
                    NormalStatusCodeURL.write(PreparedURL + '\n')
                if response.status_code == 302:
                    RedirectStatusCodeURL.write(PreparedURL + '\n')
                if response.status_code == 403:
                    ForbiddenStatusCodeURL.write(PreparedURL + '\n')
                if response.status_code == 500:
                    InternalServerError.write(PreparedURL + '\n')
            except requests.exceptions.Timeout:
                print(f"This URL {PreparedURL} is TimeOut")
            except requests.exceptions.RequestException:
                print(f"This URL {PreparedURL} throws RequestException")



if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Subdomain Validator")
    parser.add_argument('filename',help='Mention your filename')
    args = parser.parse_args()
    main(args.filename)
